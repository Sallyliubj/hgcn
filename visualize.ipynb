{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import scipy.sparse as sp\n",
    "import os\n",
    "\n",
    "HGCN_HOME= os.getcwd()\n",
    "print(HGCN_HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train\n",
    "import args\n",
    "from config import parser\n",
    "from utils.data_utils import load_data, load_inference_data, clean_and_process_nexus\n",
    "from utils.tree.load_parameters import generate_data\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tree.load_parameters import generate_data, get_labels\n",
    "from utils.tree.load_parameters import one_hot\n",
    "from utils.tree.data_utils import simulate_seq, simulate_seq_all\n",
    "from utils.tree.tree_utils import create_tree\n",
    "from utils.tree.sem import randomt_tree\n",
    "import utils.tree.computations as cmp\n",
    "from utils.tree.load_parameters import build_weighted_adjacency_matrix\n",
    "from utils.tree.substitution_models import JukesCantor\n",
    "from utils.tree.mst_utils import bifurcate_mst, get_mst\n",
    "\n",
    "import train\n",
    "import args\n",
    "from config import parser\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to generate interiior nodes given tree structure and leaf sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "def generate_interior_node(tree, leaf_features, evo_model):\n",
    "    n_nodes = len(tree)\n",
    "    n_leaves = len(leaf_features)\n",
    "\n",
    "    # Initialize node features for all nodes (only leaf nodes have initial features)\n",
    "    node_features = {i: leaf_features[i] for i in range(n_leaves)}\n",
    "    for i in range(n_leaves, n_nodes):\n",
    "        node_features[i] = np.zeros(leaf_features.shape[1])\n",
    "\n",
    "    # Create a parent map to keep track of parent-child relationships\n",
    "    parent_map = {}\n",
    "    root = max(tree.nodes)\n",
    "\n",
    "    # Assuming the tree is undirected, build parent-child relationships to identify tree structure\n",
    "    stack = [root]\n",
    "    visited = set()\n",
    "    visited.add(root)\n",
    "\n",
    "    while stack:\n",
    "        node = stack.pop()\n",
    "        children = list(tree.neighbors(node))\n",
    "        for child in children:\n",
    "            if child not in visited:\n",
    "                parent_map[child] = node\n",
    "                stack.append(child)\n",
    "                visited.add(child)\n",
    "\n",
    "    # Postorder Traversal to Collect Parameters (Iterative version)\n",
    "    c_params = np.zeros(n_nodes)\n",
    "    d_params = np.zeros((n_nodes, leaf_features.shape[1]))\n",
    "    visited.clear()\n",
    "\n",
    "    # Using a stack for iterative postorder traversal\n",
    "    stack = deque()\n",
    "    stack.append((root, False))  # Start from the root\n",
    "\n",
    "    while stack:\n",
    "        node, processed = stack.pop()\n",
    "\n",
    "        if processed:\n",
    "            # Process the node after its children have been processed\n",
    "            if node not in visited:\n",
    "                children = [child for child in tree.neighbors(node) if child != parent_map.get(node)]\n",
    "                for child in children:\n",
    "                    c_params[node] += 1 + c_params[child]\n",
    "                    d_params[node] += d_params[child]\n",
    "                visited.add(node)\n",
    "        else:\n",
    "            # Push the node back to be processed after its children\n",
    "            stack.append((node, True))\n",
    "            # Push all children to be processed first\n",
    "            children = [child for child in tree.neighbors(node) if child != parent_map.get(node)]\n",
    "            for child in children:\n",
    "                if child not in visited:\n",
    "                    stack.append((child, False))\n",
    "\n",
    "    # Preorder Traversal to Generate Features for Interior Nodes (Iterative version)\n",
    "    stack = deque([root])  # Start from the root\n",
    "    visited.clear()\n",
    "\n",
    "    while stack:\n",
    "        node = stack.pop()\n",
    "        if node not in visited:\n",
    "            # Generate node feature for the root or other nodes\n",
    "            if node == root:\n",
    "                node_features[node] = d_params[node] / (1 + c_params[node])\n",
    "            else:\n",
    "                parent_node = parent_map[node]\n",
    "                node_features[node] = (c_params[node] * node_features[parent_node] + d_params[node]) / (c_params[node] + 1)\n",
    "\n",
    "            # Mark the node as visited\n",
    "            visited.add(node)\n",
    "\n",
    "            # Add children to the stack\n",
    "            children = [child for child in tree.neighbors(node) if child != parent_map.get(node)]\n",
    "            for child in children:\n",
    "                if child not in visited:\n",
    "                    stack.append(child)\n",
    "\n",
    "    # Convert the node features to a numpy array\n",
    "    all_sequences = []\n",
    "    for i in range(n_nodes):\n",
    "        all_sequences.append(node_features[i])\n",
    "    all_sequences_np = np.array(all_sequences)\n",
    "\n",
    "    # Create Adjacency Matrix\n",
    "    adj_matrix = np.zeros((n_nodes, n_nodes), dtype=int)\n",
    "    for edge in tree.edges:\n",
    "        parent, child = edge\n",
    "        adj_matrix[parent, child] = 1\n",
    "        adj_matrix[child, parent] = 1  # Assuming undirected tree\n",
    "\n",
    "    return all_sequences_np, adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def predict_weighted_adj_matrix(leaf_features, tree, model):\n",
    "    \n",
    "    feature_tensor = torch.tensor(leaf_features, dtype=torch.float)\n",
    "    print(feature_tensor.shape)\n",
    "    \n",
    "    # Convert adjacency matrix to a PyTorch sparse tensor\n",
    "    G = nx.complete_graph(len(tree))\n",
    "    adj = nx.to_scipy_sparse_matrix(G, format='csr')\n",
    "    adj_coo = adj.tocoo()\n",
    "    indices = torch.from_numpy(np.vstack((adj_coo.row, adj_coo.col)).astype(np.int64))\n",
    "    print(indices.shape)\n",
    "\n",
    "    values = torch.from_numpy((adj_coo.data).astype(np.float32))\n",
    "    print(values.dtype)\n",
    "\n",
    "    shape = torch.Size(adj_coo.shape)\n",
    "    adj_tensor = torch.sparse.FloatTensor(indices, values, shape)\n",
    "    print(adj_tensor.dtype)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.encode(feature_tensor, adj_tensor)\n",
    "        link_probabilities = model.decode(embeddings, indices.T)\n",
    "\n",
    "    link_pred_np = np.array(link_probabilities)\n",
    "    n_nodes = adj.shape[0]\n",
    "    weighted_adj_matrix = np.zeros((n_nodes, n_nodes))\n",
    "    # Populate the adjacency matrix based on link_predictions\n",
    "    for i in range(link_pred_np.shape[0]):\n",
    "        u, v, weight = int(indices[0,i]), int(indices[1,i]), link_pred_np[i]\n",
    "        weighted_adj_matrix[u, v] = weight\n",
    "        weighted_adj_matrix[v, u] = weight \n",
    "\n",
    "    return weighted_adj_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop 1\n",
    "\n",
    "- train (e.g. 50) sequences first\n",
    "- predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop1(leaf_sequences, evo_model, model):\n",
    "    \"\"\"\n",
    "        leaf_sequences: nucleotide (ATCG)\n",
    "\n",
    "    \"\"\"\n",
    "    # encode the leaf sequences\n",
    "    encoded_seq = one_hot(leaf_sequences)\n",
    "    leaf_features = np.argmax(encoded_seq, axis=-1)\n",
    "    \n",
    "    n_leaves, n_sites = leaf_sequences.shape\n",
    "    # construct a initial random tree ontology\n",
    "    tree, opts = randomt_tree(n_leaves)\n",
    "    \n",
    "    leaves = [n for n in tree if tree.nodes[n]['type'] == 'leaf']\n",
    "    root = len(tree) - 1    \n",
    "    \n",
    "    sigma_l = 0.1  # annealing std\n",
    "    rho = 0.95  # cooling factor\n",
    "    max_iter = 100\n",
    "    T_l = tree  # init topology\n",
    "    ll_vec = []  # log-likelihood\n",
    "    Tl_vec=[]\n",
    "    max_iter = 20\n",
    "    \n",
    "   \n",
    "    for iter in range(max_iter):\n",
    "        print(f'start iter: {iter}')\n",
    "        \n",
    "        Tl_vec.append(tree)\n",
    "        up_table = cmp.compute_up_messages(leaf_sequences, tree, evo_model)\n",
    "        down_table = cmp.compute_down_messages(leaf_sequences, tree, evo_model, up_table)\n",
    "        # compute log-likelihood\n",
    "        ll_sites, ll = cmp.compute_loglikelihood(up_table, evo_model.stat_prob)\n",
    "        if len(ll_vec)>0 and ll_vec[0]>ll:#figure out argmax maybe\n",
    "            tree = Tl_vec[0]\n",
    "            ll_vec.append(ll_vec[0])\n",
    "            continue\n",
    "        ll_vec.append(ll)\n",
    "        print(f'log-likelihood: {ll}')\n",
    "\n",
    "        W, opts = cmp.compute_weight_matrix(tree, evo_model, up_table, down_table)\n",
    "        # print(\"computed weighted matrix\")\n",
    "\n",
    "        # generate interior nodes and (unweighted) adj matrix sequences given leaf sequences\n",
    "        features, adj_matrix = generate_interior_node(tree, leaf_features, evo_model)\n",
    "        print(\"generated interior nodes\")\n",
    "       \n",
    "        ## link-prediction\n",
    "        weighted_adj_matrix = predict_weighted_adj_matrix(features, tree, model)\n",
    "\n",
    "        # construct mst from weighted adj matrix\n",
    "        mst = get_mst(weighted_adj_matrix, opts)\n",
    "        # construct bifurcate tree from mst, as the tree onto for next iter\n",
    "        tree, edge = bifurcate_mst(mst, leaves, root)\n",
    "\n",
    "        sigma_l *= rho\n",
    "        if sigma_l <= 5e-3:\n",
    "            break\n",
    "        elif iter > 0 and np.abs(ll_vec[iter] - ll_vec[iter - 1]) < 1e-3:\n",
    "            break\n",
    "\n",
    "        print(f'finished iter: {iter}')\n",
    "\n",
    "    return tree, ll_vec\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.args.manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train\n",
    "import args\n",
    "from config import parser\n",
    "\n",
    "number = 1748\n",
    "# train the model on 50 simulated sequences\n",
    "filepath = f\"/Users/liubojun/Desktop/hgcn/data/inference/M{number}.nex\"\n",
    "seq = clean_and_process_nexus(filepath)\n",
    "\n",
    "num_data = 50\n",
    "n_leaves, n_features = seq.shape\n",
    "model = train.train_multiple(args.args, num_data, n_leaves, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model_path = \"model.pth\"\n",
    "\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evo_model = JukesCantor(alpha=0.1)\n",
    "\n",
    "tree, ll_vec = train_loop1(seq, evo_model, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop2(leaf_sequences, evo_model):\n",
    "    \"\"\"\n",
    "        leaf_sequences: nucleotide (ATCG)\n",
    "\n",
    "    \"\"\"\n",
    "    # encode the leaf sequences\n",
    "    encoded_seq = one_hot(leaf_sequences)\n",
    "    leaf_features = np.argmax(encoded_seq, axis=-1)\n",
    "    \n",
    "    n_leaves, n_features = leaf_sequences.shape\n",
    "    # construct a initial random tree ontology\n",
    "    tree, opts = randomt_tree(n_leaves)\n",
    "    \n",
    "    leaves = [n for n in tree if tree.nodes[n]['type'] == 'leaf']\n",
    "    root = len(tree) - 1    \n",
    "    \n",
    "    sigma_l = 0.1  # annealing std\n",
    "    rho = 0.95  # cooling factor\n",
    "    max_iter = 100\n",
    "    T_l = tree  # init topology\n",
    "    ll_vec = []  # log-likelihood\n",
    "    Tl_vec=[]\n",
    "    max_iter = 100\n",
    "    \n",
    "    print(\"start training loop\")\n",
    "    for iter in range(max_iter):\n",
    "        print(f'start iter: {iter}')\n",
    "        \n",
    "        Tl_vec.append(tree)\n",
    "        up_table = cmp.compute_up_messages(leaf_sequences, tree, evo_model)\n",
    "        down_table = cmp.compute_down_messages(leaf_sequences, tree, evo_model, up_table)\n",
    "        # compute log-likelihood\n",
    "        ll_sites, ll = cmp.compute_loglikelihood(up_table, evo_model.stat_prob)\n",
    "        if len(ll_vec)>0 and ll_vec[0]>ll:#figure out argmax maybe\n",
    "            tree = Tl_vec[0]\n",
    "            ll_vec.append(ll_vec[0])\n",
    "            continue\n",
    "        ll_vec.append(ll)\n",
    "        print(f'log-likelihood: {ll}')\n",
    "\n",
    "        W, opts = cmp.compute_weight_matrix(tree, evo_model, up_table, down_table)\n",
    "        # print(\"computed weighted matrix\")\n",
    "\n",
    "        # generate interior nodes and (unweighted) adj matrix sequences given leaf sequences\n",
    "        features, adj_matrix = generate_interior_node(tree, leaf_features, evo_model)\n",
    "        print(\"generated interior nodes\")\n",
    "        \n",
    "        # train \n",
    "        adj = sp.csr_matrix(adj_matrix) # scipy sparce matrix\n",
    "        labels = get_labels(tree)\n",
    "        data = load_inference_data(adj, features, labels, opts, args.args)\n",
    "        print(\"start training\")\n",
    "        weighted_adj_matrix, labels, opts, model = train.train(args.args, data, n_features)\n",
    "        print(\"finished training\")\n",
    "        # construct mst from weighted adj matrix\n",
    "        mst = get_mst(weighted_adj_matrix, opts)\n",
    "        # construct bifurcate tree from mst, as the tree onto for next iter\n",
    "        tree, edge = bifurcate_mst(mst, leaves, root)\n",
    "\n",
    "        sigma_l *= rho\n",
    "        if sigma_l <= 5e-3:\n",
    "            break\n",
    "        elif iter > 0 and np.abs(ll_vec[iter] - ll_vec[iter - 1]) < 1e-3:\n",
    "            break\n",
    "\n",
    "        print(f'finished iter: {iter}')\n",
    "\n",
    "    return tree, ll_vec\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1310, 1366, 1367, 1745, 1746, 1747, 1748, 1749\n",
    "numbers = [1310, 1366, 1367, 1745, 1746, 1747, 1748, 1749]\n",
    "\n",
    "for number in numbers:\n",
    "    filepath = f\"/Users/liubojun/Desktop/hgcn/data/inference/M{number}.nex\"\n",
    "    seq = clean_and_process_nexus(filepath)\n",
    "\n",
    "    # Euclidean\n",
    "    args.args.manifold = 'Euclidean'\n",
    "    manifold = args.args.manifold\n",
    "    evo_model = JukesCantor(alpha=0.1)\n",
    "    tree, ll_vec_euc = train_loop2(seq, evo_model)\n",
    "\n",
    "    # Save the NumPy array as a FASTA file\n",
    "    save_as_fasta(seq, f\"results/M{number}/{manifold}/M{number}.fasta\")\n",
    "    # save tree\n",
    "    with open(f\"results/M{number}/{manifold}/pred_tree_M{number}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(tree, f)\n",
    "    # save log-likelihood\n",
    "    np.savetxt(f'results/M{number}/{manifold}/ll_vec_M{number}.txt',ll_vec_euc)\n",
    "\n",
    "    # Hyperbolic\n",
    "    args.args.manifold = 'PoincareBall'\n",
    "    manifold = args.args.manifold\n",
    "    evo_model = JukesCantor(alpha=0.1)\n",
    "    tree, ll_vec_hyp = train_loop2(seq, evo_model)\n",
    "\n",
    "    # Save the NumPy array as a FASTA file\n",
    "    save_as_fasta(seq, f\"results/M{number}/{manifold}/M{number}.fasta\")\n",
    "    # save tree\n",
    "    with open(f\"results/M{number}/{manifold}/pred_tree_M{number}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(tree, f)\n",
    "    # save log-likelihood\n",
    "    np.savetxt(f'results/M{number}/{manifold}/ll_vec_M{number}.txt',ll_vec_hyp)\n",
    "\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(ll_vec_hyp)\n",
    "    ax.plot(ll_vec_euc)\n",
    "    ax.set(xlabel='iter', ylabel='log-likelihood', title=f\"log-likelihood for M{number}\")\n",
    "    ax.grid()\n",
    "    ax.legend([\"hyperbolic\", \"euclidean\"])\n",
    "    fig.savefig(f\"results/M{number}/ll_plot_M{number}.png\") \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1310, 1366, 1367, 1745, 1746, 1747, 1748, 1749\n",
    "numbers = [1310, 1366, 1367, 1745, 1746, 1747, 1748, 1749]\n",
    "\n",
    "for number in numbers:\n",
    "    filepath = f\"/Users/liubojun/Desktop/hgcn/data/inference/M{number}.nex\"\n",
    "    seq = clean_and_process_nexus(filepath)\n",
    "\n",
    "    # Euclidean\n",
    "    args.args.manifold = 'Euclidean'\n",
    "    manifold = args.args.manifold\n",
    "\n",
    "    # Save the NumPy array as a FASTA file\n",
    "    save_as_fasta(seq, f\"results/M{number}/{manifold}/M{number}.fasta\")\n",
    "\n",
    "    # Hyperbolic\n",
    "    args.args.manifold = 'PoincareBall'\n",
    "    manifold = args.args.manifold\n",
    "    \n",
    "    # Save the NumPy array as a FASTA file\n",
    "    save_as_fasta(seq, f\"results/M{number}/{manifold}/M{number}.fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(ll_vec_hyp)\n",
    "ax.plot(ll_vec_euc)\n",
    "ax.set(xlabel='iter', ylabel='log-likelihood', title=f\"log-likelihood for M{number}\")\n",
    "ax.grid()\n",
    "ax.legend([\"hyperbolic\", \"euclidean\"])\n",
    "fig.savefig(f\"results/M{number}/ll_plot_M{number}.png\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoincareBall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.args.manifold = 'Euclidean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifold = args.args.manifold\n",
    "manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_leaves = 50\n",
    "# tree, opts = randomt_tree(n_leaves)\n",
    "# sim_seq = simulate_seq_all(tree, evo_model, ndata=len(tree))\n",
    "\n",
    "evo_model = JukesCantor(alpha=0.1)\n",
    "tree, ll_vec = train_loop2(seq, evo_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save leaf sequences, predicted tree, and log-likelihood "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifold = args.args.manifold\n",
    "\n",
    "# convert to fasta\n",
    "def save_as_fasta(sequences, file_name):\n",
    "    directory = os.path.dirname(file_name)\n",
    "    if directory and not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    with open(file_name, 'w') as f:\n",
    "        for i, seq in enumerate(sequences):\n",
    "            # Write the header line\n",
    "            f.write(f\">Sequence_{i+1}\\n\")\n",
    "            # Write the sequence line (convert array to a single string)\n",
    "            f.write(\"\".join(seq) + \"\\n\")\n",
    "\n",
    "# Save the NumPy array as a FASTA file\n",
    "save_as_fasta(sim_seq, f\"results/M{number}/{manifold}/M{number}.fasta\")\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(f\"results/M{number}/{manifold}/pred_tree_M{number}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tree, f)\n",
    "\n",
    "# Load the saved NetworkX graph from the pickle file\n",
    "# with open(\"mst_tree.pkl\", \"rb\") as f:\n",
    "#     mst_loaded = pickle.load(f)\n",
    "\n",
    "# save log-likelihood\n",
    "np.savetxt(f'results/M{number}/{manifold}/ll_vec_M{number}.txt',ll_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_vec_euc = ll_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ll_vec_hyp)\n",
    "plt.plot(ll_vec_euc)\n",
    "plt.xlabel('iter')\n",
    "plt.ylabel('log-likelihood')\n",
    "plt.grid()\n",
    "plt.title(\"log-likelihood\")\n",
    "plt.legend([\"hyperbolic\", \"euclidean\"])\n",
    "plt.savefig(f\"results/M{number}/ll_plot_M{number}.png\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_leaves = 20\n",
    "tree, opts = randomt_tree(n_leaves)\n",
    "evo_model = JukesCantor(alpha=0.1)\n",
    "sim_seq = simulate_seq_all(tree, evo_model, ndata=len(tree))\n",
    "\n",
    "tree, ll_vec = train_loop2(sim_seq, evo_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ll_vec_euc)\n",
    "plt.xlabel('iter')\n",
    "plt.ylabel('log-likelihood')\n",
    "plt.title(\"Euclidean log-likelihood\")\n",
    "plt.grid()\n",
    "plt.xlabel(\"iter\")\n",
    "plt.ylabel(\"lig-likelihood\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to fasta\n",
    "def save_as_fasta(sequences, file_name):\n",
    "    with open(file_name, 'w') as f:\n",
    "        for i, seq in enumerate(sequences):\n",
    "            # Write the header line\n",
    "            f.write(f\">Sequence_{i+1}\\n\")\n",
    "            # Write the sequence line (convert array to a single string)\n",
    "            f.write(\"\".join(seq) + \"\\n\")\n",
    "\n",
    "# Save the NumPy array as a FASTA file\n",
    "save_as_fasta(sim_seq, \"euc_simulated_sequences.fasta\")\n",
    "\n",
    "\n",
    "import pickle\n",
    "with open(\"euc_predicted_tree.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tree, f)\n",
    "\n",
    "# save log-likelihood\n",
    "np.savetxt('euc_ll_vec.txt',ll_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tree.sem import randomt_tree, main\n",
    "\n",
    "tree, opts, leaf_sequences, ll_vec_init = main(n_leaves=20, ndata=39)\n",
    "\n",
    "# encode the leaf sequences\n",
    "encoded_seq = one_hot(leaf_sequences)\n",
    "leaf_features = np.argmax(encoded_seq, axis=-1)\n",
    "\n",
    "n_leaves, n_sites = leaf_sequences.shape\n",
    "# construct a initial random tree ontology\n",
    "# tree, opts = randomt_tree(n_leaves)\n",
    "\n",
    "leaves = [n for n in tree if tree.nodes[n]['type'] == 'leaf']\n",
    "root = len(tree) - 1    \n",
    "\n",
    "sigma_l = 0.1  # annealing std\n",
    "rho = 0.95  # cooling factor\n",
    "max_iter = 100\n",
    "T_l = tree  # init topology\n",
    "ll_vec = []  # log-likelihood\n",
    "Tl_vec=[]\n",
    "max_iter = 100\n",
    "\n",
    "print(\"start training loop\")\n",
    "for iter in range(max_iter):\n",
    "    print(f'start iter: {iter}')\n",
    "    \n",
    "    Tl_vec.append(tree)\n",
    "    up_table = cmp.compute_up_messages(leaf_sequences, tree, evo_model)\n",
    "    down_table = cmp.compute_down_messages(leaf_sequences, tree, evo_model, up_table)\n",
    "    # compute log-likelihood\n",
    "    ll_sites, ll = cmp.compute_loglikelihood(up_table, evo_model.stat_prob)\n",
    "    if len(ll_vec)>0 and ll_vec[0]>ll:#figure out argmax maybe\n",
    "        tree = Tl_vec[0]\n",
    "        ll_vec.append(ll_vec[0])\n",
    "        continue\n",
    "    ll_vec.append(ll)\n",
    "    print(f'log-likelihood: {ll}')\n",
    "\n",
    "    W, opts = cmp.compute_weight_matrix(tree, evo_model, up_table, down_table)\n",
    "    # print(\"computed weighted matrix\")\n",
    "\n",
    "    # generate interior nodes and (unweighted) adj matrix sequences given leaf sequences\n",
    "    features, adj_matrix = generate_interior_node(tree, leaf_features, evo_model)\n",
    "    print(\"generated interior nodes\")\n",
    "    \n",
    "    # train \n",
    "    adj = sp.csr_matrix(adj_matrix) # scipy sparce matrix\n",
    "    labels = get_labels(tree)\n",
    "    data = load_inference_data(adj, features, labels, opts, args.args)\n",
    "    print(\"start training\")\n",
    "    weighted_adj_matrix, labels, opts, model = train.train(args.args, dat, n_features)\n",
    "    print(\"finished training\")\n",
    "    # construct mst from weighted adj matrix\n",
    "    mst = get_mst(weighted_adj_matrix, opts)\n",
    "    # construct bifurcate tree from mst, as the tree onto for next iter\n",
    "    tree, edge = bifurcate_mst(mst, leaves, root)\n",
    "\n",
    "    sigma_l *= rho\n",
    "    if sigma_l <= 5e-3:\n",
    "        break\n",
    "    elif iter > 0 and np.abs(ll_vec[iter] - ll_vec[iter - 1]) < 1e-3:\n",
    "        break\n",
    "\n",
    "    print(f'finished iter: {iter}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. On Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/Users/liubojun/Desktop/hgcn/logs/lp/2024_11_20/1/'\n",
    "adj_matrix = np.load(os.path.join(save_path,\"adj_matrix.npy\"), allow_pickle=True)\n",
    "adj_matrix = adj_matrix.item() \n",
    "# adj_matrix = (adj_matrix.item()).toarray()\n",
    "features = np.load(os.path.join(save_path,\"feats.npz\"))\n",
    "features = features['features']\n",
    "\n",
    "embeddings = np.load(os.path.join(save_path,\"embeddings.npy\"))\n",
    "labels = np.loadtxt(os.path.join(save_path, \"labels.txt\"))\n",
    "opt = np.loadtxt(os.path.join(save_path, \"opt.txt\"))\n",
    "\n",
    "arg_path = os.path.join(save_path,\"args.pkl\")\n",
    "print(arg_path)\n",
    "with open(arg_path, 'rb') as f:\n",
    "    args = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for traing data\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.base_models import NCModel, LPModel\n",
    "\n",
    "# Instantiate the model (you need to use the same architecture that was trained)\n",
    "Model = LPModel(args)  # Replace with the appropriate model class\n",
    "\n",
    "# Load the trained model parameters\n",
    "Model.load_state_dict(torch.load(model_dir))\n",
    "Model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tree.load_parameters import generate_data\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert feature matrix to a PyTorch tensor\n",
    "feature_tensor = torch.tensor(features, dtype=torch.float)\n",
    "print(feature_tensor.shape)\n",
    "print(adj_matrix.shape)\n",
    "\n",
    "# Convert adjacency matrix to a PyTorch sparse tensor\n",
    "adj_coo = adj_matrix.tocoo()\n",
    "indices = torch.from_numpy(np.vstack((adj_coo.row, adj_coo.col)).astype(np.int64))\n",
    "print(indices.shape)\n",
    "\n",
    "values = torch.from_numpy((adj_coo.data).astype(np.float32))\n",
    "print(values.dtype)\n",
    "\n",
    "shape = torch.Size(adj_coo.shape)\n",
    "adj_tensor = torch.sparse.FloatTensor(indices, values, shape)\n",
    "print(adj_tensor.dtype)\n",
    "\n",
    "# embeddings_tensor = torch.tensor(embeddings, dtype=torch.float)\n",
    "# print(embeddings_tensor.shape)\n",
    "\n",
    "# Generate node embeddings using the model's `encode()` function\n",
    "with torch.no_grad():\n",
    "    embeddings = Model.encode(feature_tensor, adj_tensor)\n",
    "    link_probabilities = Model.decode(embeddings, indices.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_probabilities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_np = indices.cpu().numpy().T  # Shape: (n_edges, 2)\n",
    "link_probabilities_np = link_probabilities.cpu().numpy()  # Shape: (n_edges,)\n",
    "\n",
    "# Create a list of tuples to store node pairs and their corresponding probabilities\n",
    "link_predictions = []\n",
    "count = 0\n",
    "for i in range(len(indices_np)):\n",
    "    node_pair = (indices_np[i][0], indices_np[i][1])  # Extract node pair (u, v)\n",
    "    probability = link_probabilities_np[i]  # Extract predicted probability\n",
    "    link_predictions.append((node_pair[0], node_pair[1], probability))\n",
    "    if probability >  0.00001:\n",
    "        count += 1\n",
    "print(count)\n",
    "\n",
    "# Print the matched node pairs and link probabilities\n",
    "for node1, node2, prob in link_predictions:\n",
    "    print(f\"Node Pair ({node1}, {node2}): Link Probability = {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_pred_np = np.array(link_predictions)\n",
    "# Initialize an n_nodes x n_nodes adjacency matrix with zeros\n",
    "n_nodes = adj_matrix.shape[0]\n",
    "weighted_adj_matrix = np.zeros((n_nodes, n_nodes))\n",
    "# Populate the adjacency matrix based on link_predictions\n",
    "for i in range(link_pred_np.shape[0]):\n",
    "    u, v, weight = int(link_pred_np[i, 0]), int(link_pred_np[i, 1]), link_pred_np[i, 2]\n",
    "    weighted_adj_matrix[u, v] = weight\n",
    "    weighted_adj_matrix[v, u] = weight  # Assuming the graph is undirected\n",
    "\n",
    "# Print the resulting weighted adjacency matrix\n",
    "print(\"Weighted Adjacency Matrix:\")\n",
    "print(weighted_adj_matrix[100][102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_pred_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "def plot_graph(graph):\n",
    "    # Generate layout for the graph nodes\n",
    "    pos = nx.spring_layout(graph)  # You can also use other layouts like nx.kamada_kawai_layout(graph)\n",
    "\n",
    "    # Draw the nodes\n",
    "    nx.draw_networkx_nodes(graph, pos, node_size=50)\n",
    "\n",
    "    # Draw the edges\n",
    "    nx.draw_networkx_edges(graph, pos, edgelist=graph.edges(), width=1)\n",
    "\n",
    "    # Draw the labels for nodes and edges\n",
    "    nx.draw_networkx_labels(graph, pos, font_size=5, font_family='sans-serif')\n",
    "    edge_labels = nx.get_edge_attributes(graph, 'weight')\n",
    "    # nx.draw_networkx_edge_labels(graph, pos, edge_labels=edge_labels)\n",
    "\n",
    "    # Display the graph\n",
    "    plt.title(\"Tree\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tree.mst_utils import bifurcate_mst, get_mst\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mst = get_mst(weighted_adj_matrix, opt)\n",
    "plot_graph(mst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = [node for node in mst.nodes if labels[node] == 0]\n",
    "leaves = [node for node in mst.nodes if labels[node] == 1]\n",
    "\n",
    "bifurcate_tree, edge = bifurcate_mst(mst, leaves, 198)\n",
    "\n",
    "bifurcate_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(bifurcate_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. On Simulated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tree.load_parameters import generate_data\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "adj_matrix, features, labels = generate_data(n_leaves=100, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert feature matrix to a PyTorch tensor\n",
    "feature_tensor = torch.tensor(features, dtype=torch.float)\n",
    "print(feature_tensor.shape)\n",
    "\n",
    "# Convert adjacency matrix to a PyTorch sparse tensor\n",
    "adj_coo = adj_matrix.tocoo()\n",
    "indices = torch.from_numpy(np.vstack((adj_coo.row, adj_coo.col)).astype(np.int64))\n",
    "print(indices.dtype)\n",
    "\n",
    "values = torch.from_numpy((adj_coo.data).astype(np.float32))\n",
    "print(values.dtype)\n",
    "\n",
    "shape = torch.Size(adj_coo.shape)\n",
    "adj_tensor = torch.sparse.FloatTensor(indices, values, shape)\n",
    "print(adj_tensor.dtype)\n",
    "\n",
    "embeddings_tensor = torch.tensor(embeddings, dtype=torch.float)\n",
    "\n",
    "# Generate node embeddings using the model's `encode()` function\n",
    "with torch.no_grad():\n",
    "    link_probabilities = Model.decode(embeddings_tensor, indices.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_pairs = np.array(sp.find(adj_matrix)[:2]).T\n",
    "\n",
    "n_nodes = adj_matrix.shape[0]  # Number of nodes\n",
    "\n",
    "# Create a set of all possible pairs of nodes\n",
    "all_pairs = set((i, j) for i in range(n_nodes) for j in range(i + 1, n_nodes))\n",
    "\n",
    "# Create a set of all positive pairs\n",
    "positive_pairs_set = set(tuple(pair) for pair in positive_pairs)\n",
    "\n",
    "# Generate negative pairs by taking the difference between all possible pairs and positive pairs\n",
    "negative_pairs_set = all_pairs - positive_pairs_set\n",
    "\n",
    "# Convert the negative pairs set to a numpy array\n",
    "negative_pairs = np.array(list(negative_pairs_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_labels = np.ones(len(positive_pairs))\n",
    "negative_labels = np.zeros(len(negative_pairs))\n",
    "\n",
    "# Combine pairs and labels\n",
    "node_pairs = np.vstack([positive_pairs, negative_pairs])\n",
    "labels = np.hstack([positive_labels, negative_labels])\n",
    "\n",
    "len(node_pairs), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_tensor = torch.tensor(embeddings, dtype=torch.float)\n",
    "node_pairs_tensor = torch.tensor(node_pairs, dtype=torch.long)\n",
    "\n",
    "u_embeddings = embeddings_tensor[node_pairs_tensor[:, 0]]  # Embeddings for the first node in each pair\n",
    "v_embeddings = embeddings_tensor[node_pairs_tensor[:, 1]]  # Embeddings for the second node in each pair\n",
    "scores = torch.sum(u_embeddings * v_embeddings, dim=1)\n",
    "\n",
    "threshold = 0.9  # Adjust as needed\n",
    "predicted_links = scores > threshold\n",
    "\n",
    "len(predicted_links==1)\n",
    "\n",
    "(predicted_links==True).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Predicted Links on Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create an empty graph\n",
    "G = nx.Graph()  # Use nx.DiGraph() if you want a directed graph\n",
    "\n",
    "# Add nodes (optional, NetworkX will add nodes automatically when adding edges)\n",
    "nodes = np.unique(node_pairs)  # Get unique nodes\n",
    "G.add_nodes_from(nodes)\n",
    "\n",
    "# Add edges based on predicted links\n",
    "for i, (u, v) in enumerate(node_pairs):\n",
    "    if predicted_links[i]:\n",
    "        G.add_edge(u, v, weight=scores[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the layout for the graph (e.g., spring_layout, circular_layout, random_layout)\n",
    "pos = nx.spring_layout(G)  # Positions the nodes for visualization\n",
    "\n",
    "# Draw the graph\n",
    "plt.figure(figsize=(10, 8))\n",
    "nx.draw(\n",
    "    G, pos,\n",
    "    with_labels=True,\n",
    "    node_color='skyblue',\n",
    "    edge_color='gray',\n",
    "    node_size=100,\n",
    "    font_size=10,\n",
    "    font_weight='bold'\n",
    ")\n",
    "\n",
    "plt.title(\"Predicted Graph Visualization\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum Spanning Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new graph with negated weights\n",
    "G_neg = nx.Graph()\n",
    "for u, v, data in G.edges(data=True):\n",
    "    G_neg.add_edge(u, v, weight=-data['weight'])  # Negate the weights\n",
    "\n",
    "# Use NetworkX to find the minimum spanning tree (which, with negated weights, is equivalent to MST)\n",
    "mst_neg = nx.minimum_spanning_tree(G_neg)\n",
    "\n",
    "# Convert the MST back to positive weights for the actual Maximum Spanning Tree\n",
    "mst = nx.Graph()\n",
    "for u, v, data in mst_neg.edges(data=True):\n",
    "    mst.add_edge(u, v, weight=-data['weight'])  # Revert to original weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the layout for visualization\n",
    "pos = nx.spring_layout(mst)  # Positioning the nodes for visualization\n",
    "\n",
    "# Draw the MST\n",
    "plt.figure(figsize=(10, 8))\n",
    "nx.draw(\n",
    "    mst, pos,\n",
    "    with_labels=True,\n",
    "    node_color='lightblue',\n",
    "    edge_color='gray',\n",
    "    node_size=800,\n",
    "    font_size=10,\n",
    "    font_weight='bold'\n",
    ")\n",
    "\n",
    "# Draw edge labels with weights\n",
    "edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels)\n",
    "\n",
    "plt.title(\"Maximum Spanning Tree Visualization\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hgcn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
